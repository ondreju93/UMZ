{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tikzmagic",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-4fa66f4ce51c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'load_ext tikzmagic'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/andrzej/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2334\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2335\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2336\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2338\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/andrzej/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2255\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2256\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2257\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2258\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-64>\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n",
      "\u001b[1;32m/home/andrzej/.local/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/andrzej/.local/lib/python2.7/site-packages/IPython/core/magics/extension.pyc\u001b[0m in \u001b[0;36mload_ext\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Missing module name.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'already loaded'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/andrzej/.local/lib/python2.7/site-packages/IPython/core/extensions.pyc\u001b[0m in \u001b[0;36mload_extension\u001b[1;34m(self, module_str)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_load_ipython_extension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named tikzmagic"
     ]
    }
   ],
   "source": [
    "%load_ext tikzmagic\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = \"\"\"\n",
    "<style>\n",
    ".output_png { text-align:  center; }\n",
    "</style>\n",
    "    \"\"\"\n",
    "    return HTML(styles)\n",
    "css_styling()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sieci neuronowe\n",
    "##Metody uczenia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Trochę historii: Perceptron liniowy\n",
    "\n",
    "**Mark 1 perceptron** (Frank Rosenblatt, 1957): \n",
    "* Aparat przeznaczony do rozpoznawania obrazu;\n",
    "* 400 fotokomórek\n",
    "* Wagi to potencjometry;\n",
    "* Wagi aktualizowana za pomocą silniczków.\n",
    "\n",
    "The New York Times, 1958:\n",
    "> [...] the embryo of an electronic computer that the Navy expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img style=\"margin: auto\" width=\"80%\" src=\"http://m.natemat.pl/b94a41cd7322e1b8793e4644e5f82683,641,0,0,0.png\" alt=\"Frank Rosenblatt\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img style=\"margin: auto\" src=\"http://m.natemat.pl/02943a7dc0f638d786b78cd5c9e75742,641,0,0,0.png\" width=\"70%\" alt=\"Frank Rosenblatt\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "input_collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img style=\"margin:auto\" src=\"http://www.newyorker.com/wp-content/uploads/2012/11/frank-rosenblatt-perception.jpg\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "input_collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('cNxadbrN_aI', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Uczenie (bez ingerencji człowieka)\n",
    "\n",
    "Cykl uczenia perceptronu (w sumie 2000 \"epok\"):\n",
    "* pokazanie (do kamery cyfrowej) planszy z kolejnym obiektem (np. trójkat, koło, kwadrat,...);\n",
    "* zaobserwowanie, jaka lampka się zapaliła na wyjściu;\n",
    "* sprawdzenie, czy jest to właściwa lampka (arbitralnie ustalona);\n",
    "* wysłanie sygnału \"nagrody\" lub \"kary\".\n",
    "* Człowiek tylko \"podaje\" informacje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Perceptron: formalizacja\n",
    "\n",
    "Nieliniowa funkcja aktywacji (Rosenblatt: funkcja schodkowa):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "input_collapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "x = [-1,-.23,1] \n",
    "y = [-1, -1, 1]\n",
    "plt.ylim(-1.2,1.2)\n",
    "plt.xlim(-1.2,1.2)\n",
    "plt.plot([-2,2],[1,1], color='black', ls=\"dashed\")\n",
    "plt.plot([-2,2],[-1,-1], color='black', ls=\"dashed\")\n",
    "plt.step(x, y, lw=3)\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.spines['bottom'].set_position(('data',0))\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.spines['left'].set_position(('data',0))\n",
    "\n",
    "plt.annotate(r'$\\theta_0$',\n",
    "             xy=(-.23,0), xycoords='data',\n",
    "             xytext=(-50, +50), textcoords='offset points', fontsize=26,\n",
    "             arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "input_collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$ g(z) = \\left\\{ \n",
    "\\begin{array}{rl}\n",
    "1  & \\textrm{gdy $z > \\theta_0$} \\\\\n",
    "-1 & \\textrm{wpp.}\n",
    "\\end{array}\n",
    "\\right. $$\n",
    "\n",
    "gdzie $z = \\theta_0x_0 + \\ldots + \\theta_nx_n$. Niech $\\theta_0$ to próg aktywacji, ustalamy $x_0 = 1$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Składamy wszystko w całość"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "input_collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%tikz -l arrows,automata,positioning,shapes,shapes.geometric,fit -f png -s 2000,1400\n",
    "\\tikzstyle{every node}=[font=\\large]\n",
    "\\tikzstyle{every path}=[line width=1pt]\n",
    "\n",
    "\\node[state] (x0) {$1$};\n",
    "\\node[state] (x1) [below=0.5cm of x0] {$x_1$};\n",
    "\\node[state] (x2) [below=0.5cm of x1] {$x_2$};\n",
    "\\node[state, draw=none,fill=none] (dots) [below=0.5cm of x2] {$\\cdots$};\n",
    "\\node[state] (xn) [below=0.5cm of dots] {$x_n$};\n",
    "\n",
    "\\node[state,circle,label=above:{Funkcja wej\\'{s}cia}] (sum) [right=2cm of x2] {$z=\\displaystyle\\sum_{i=0}^{n}\\theta_ix_i$};\n",
    "\\node[state,rectangle,label=above:{Funkcja aktywacji}] (g) [right=of sum] \n",
    "{$g(z) = \\left\\{\\begin{array}{rl} 1 & \\textrm{gdy } z > \\theta_0 \\\\ -1 & \\textrm{wpp.} \\end{array}\\right.$};\n",
    "\\node[state] (output) [right=of g]  {Wyj\\'{s}cie};\n",
    "\n",
    "\n",
    "\\path[->] \n",
    "(x0) edge node [above, pos=0.4] {$\\theta_0$} (sum)\n",
    "(x1) edge node [above, pos=0.4] {$\\theta_1$} (sum)\n",
    "(x2) edge node [above, pos=0.4] {$\\theta_2$} (sum)\n",
    "(xn) edge node [above, pos=0.4] {$\\theta_n$} (sum)\n",
    "(sum) edge node {} (g)\n",
    "(g) edge node {} (output);\n",
    " \n",
    "\\node [draw,dashed, fit= (x0) (x1) (x2) (dots) (xn),label=above:Cechy, label=below:{Warstwa 0}] {};\n",
    "\\node [draw,dashed, fit= (sum) (g) (output),label=above:Neuron, label=below:{Warstwa 1}, inner sep=0.65cm] {};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "input_collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pseudokod\n",
    "\n",
    "* Ustal wartości początkowe $\\theta$ (wektor 0 lub liczby losowe blisko 0)\n",
    "* Dla każdego przykładu $(x^{(i)}, y^{(i)})$, dla $i=1,\\ldots,m$\n",
    "    * Oblicz wartość wyjścia $o^{(i)}$:\n",
    "    $$o^{(i)} = g(\\theta^{T}x^{(i)}) = g(\\sum_{j=0}^{n} \\theta_jx_j^{(i)})$$\n",
    "    * Wykonaj aktualizację wag (tzw. ***perceptron rule***):\n",
    "    $$ \\theta := \\theta + \\Delta \\theta $$\n",
    "    $$ \\Delta \\theta = \\alpha(y^{(i)}-o^{(i)})x^{(i)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Pytania:**\n",
    "* Co nam to przypomina?\n",
    "* Jakie wartości może przyjąć wyrażenie $\\Delta \\theta_j$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Reguła perceptronowa: **\n",
    "\n",
    "$$\\theta_j := \\theta_j + \\Delta \\theta_j $$\n",
    "\n",
    "**Poprawnie zaklasyfikowane**:\n",
    "\n",
    "* $y^{(i)}=1$ oraz $o^{(i)}=1$ : $$\\Delta\\theta_j = \\alpha(1 - 1)x_j^{(i)} = 0$$\n",
    "* $y^{(i)}=-1$ oraz $o^{(i)}=-1$ : $$\\Delta\\theta_j = \\alpha(-1 - -1)x_j^{(i)} = 0$$\n",
    "\n",
    "Skoro trafiłeś, to nic nie zmieniaj!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Reguła perceptronowa: **\n",
    "\n",
    "$$\\theta_j := \\theta_j + \\Delta \\theta_j $$\n",
    "\n",
    "**Niepoprawnie zaklasyfikowane**:\n",
    "\n",
    "* $y^{(i)}=1$ oraz $o^{(i)}=-1$ : $$\\Delta\\theta_j = \\alpha(1 - -1)x_j^{(i)} = 2 \\alpha x_j^{(i)}$$\n",
    "* $y^{(i)}=-1$ oraz $o^{(i)}=1$ : $$\\Delta\\theta_j = \\alpha(-1 - 1)x_j^{(i)} = -2 \\alpha x_j^{(i)}$$\n",
    "\n",
    "Przesuń w wagi w odpowiednią stronę:\n",
    "* Czyli zmniejsz jeśli, niepoprawnie przekroczono próg; \n",
    "* Zwiększ, jeśli nie przekroczono."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Zalety:\n",
    "* Dosyć intuicyjny i prosty\n",
    "* Łatwa implementacja\n",
    "* Wykazano, że konwerguje w skończonym czasie, gdy dane można linowo oddzielić."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Wady:\n",
    "* Może \"skakać\" w nieskończoność dla danych, których nie da się oddzielić liniowo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "input_collapsed": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(121)\n",
    "x = [-2,-.23,2] \n",
    "y = [-1, -1, 1]\n",
    "plt.ylim(-1.2,1.2)\n",
    "plt.xlim(-2.2,2.2)\n",
    "plt.plot([-2,2],[1,1], color='black', ls=\"dashed\")\n",
    "plt.plot([-2,2],[-1,-1], color='black', ls=\"dashed\")\n",
    "plt.step(x, y, lw=3)\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.spines['bottom'].set_position(('data',0))\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.spines['left'].set_position(('data',0))\n",
    "\n",
    "plt.annotate(r'$\\theta_0$',\n",
    "             xy=(-.23,0), xycoords='data',\n",
    "             xytext=(-50, +50), textcoords='offset points', fontsize=26,\n",
    "             arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plt.subplot(122)\n",
    "x2 = np.linspace(-2,2,100)\n",
    "y2 = np.tanh(x2+ 0.23)\n",
    "plt.ylim(-1.2,1.2)\n",
    "plt.xlim(-2.2,2.2)\n",
    "plt.plot([-2,2],[1,1], color='black', ls=\"dashed\")\n",
    "plt.plot([-2,2],[-1,-1], color='black', ls=\"dashed\")\n",
    "plt.plot(x2, y2, lw=3)\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.spines['bottom'].set_position(('data',0))\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "ax.spines['left'].set_position(('data',0))\n",
    "\n",
    "plt.annotate(r'$\\theta_0$',\n",
    "             xy=(-.23,0), xycoords='data',\n",
    "             xytext=(-50, +50), textcoords='offset points', fontsize=26,\n",
    "             arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Rozwiązanie: SGD!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "input_collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Perceptron a regresja liniowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "input_collapsed": true
   },
   "outputs": [],
   "source": [
    "%%tikz -l arrows,automata,positioning,shapes,shapes.geometric,fit -f png -s 2000,1400\n",
    "\\tikzstyle{every node}=[font=\\large]\n",
    "\\tikzstyle{every path}=[line width=1pt]\n",
    "\n",
    "\\node[state] (x0) {$1$};\n",
    "\\node[state] (x1) [below=0.5cm of x0] {$x_1$};\n",
    "\\node[state] (x2) [below=0.5cm of x1] {$x_2$};\n",
    "\\node[state, draw=none,fill=none] (dots) [below=0.5cm of x2] {$\\cdots$};\n",
    "\\node[state] (xn) [below=0.5cm of dots] {$x_n$};\n",
    "\n",
    "\\node[state,circle,label=above:{Funkcja wej\\'{s}cia}] (sum) [right=2cm of x2] {$z=\\displaystyle\\sum_{i=0}^{n}\\theta_ix_i$};\n",
    "\\node[state,rectangle,label=above:{Funkcja aktywacji}] (g) [right=of sum] \n",
    "{$g(z) = z$};\n",
    "\\node[state] (output) [right=of g]  {Wyj\\'{s}cie};\n",
    "\n",
    "\n",
    "\\path[->] \n",
    "(x0) edge node [above, pos=0.4] {$\\theta_0$} (sum)\n",
    "(x1) edge node [above, pos=0.4] {$\\theta_1$} (sum)\n",
    "(x2) edge node [above, pos=0.4] {$\\theta_2$} (sum)\n",
    "(xn) edge node [above, pos=0.4] {$\\theta_n$} (sum)\n",
    "(sum) edge node {} (g)\n",
    "(g) edge node {} (output);\n",
    " \n",
    "\\node [draw,dashed, fit= (x0) (x1) (x2) (dots) (xn),label=above:Cechy, label=below:{Warstwa 0}] {};\n",
    "\\node [draw,dashed, fit= (sum) (g) (output),label=above:Neuron, label=below:{Warstwa 1}, inner sep=0.65cm] {};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Uczenie regresji liniowej:\n",
    "* Model: $$h_{\\theta}(x) = \\sum_{i=0}^n \\theta_ix_i$$\n",
    "* Funkcja kosztu (błąd średniokwadratowy): $$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "* Po obliczeniu $\\nabla J(\\theta)$, zwykły SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "input_collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Perceptron a \n",
    "## binarna regresja logistyczna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "input_collapsed": true
   },
   "outputs": [],
   "source": [
    "%%tikz -l arrows,automata,positioning,shapes,shapes.geometric,fit -f png -s 2000,1400\n",
    "\\tikzstyle{every node}=[font=\\large]\n",
    "\\tikzstyle{every path}=[line width=1pt]\n",
    "\n",
    "\\node[state] (x0) {$1$};\n",
    "\\node[state] (x1) [below=0.5cm of x0] {$x_1$};\n",
    "\\node[state] (x2) [below=0.5cm of x1] {$x_2$};\n",
    "\\node[state, draw=none,fill=none] (dots) [below=0.5cm of x2] {$\\cdots$};\n",
    "\\node[state] (xn) [below=0.5cm of dots] {$x_n$};\n",
    "\n",
    "\\node[state,circle,label=above:{Funkcja wej\\'{s}cia}] (sum) [right=2cm of x2] {$z=\\displaystyle\\sum_{i=0}^{n}\\theta_ix_i$};\n",
    "\\node[state,rectangle,label=above:{Funkcja aktywacji}] (g) [right=of sum] \n",
    "{$g(z) = \\displaystyle\\frac{1}{1+e^{-z}}$};\n",
    "\\node[state] (output) [right=of g]  {Wyj\\'{s}cie};\n",
    "\n",
    "\n",
    "\\path[->] \n",
    "(x0) edge node [above, pos=0.4] {$\\theta_0$} (sum)\n",
    "(x1) edge node [above, pos=0.4] {$\\theta_1$} (sum)\n",
    "(x2) edge node [above, pos=0.4] {$\\theta_2$} (sum)\n",
    "(xn) edge node [above, pos=0.4] {$\\theta_n$} (sum)\n",
    "(sum) edge node {} (g)\n",
    "(g) edge node {} (output);\n",
    " \n",
    "\\node [draw,dashed, fit= (x0) (x1) (x2) (dots) (xn),label=above:Cechy, label=below:{Warstwa 0}] {};\n",
    "\\node [draw,dashed, fit= (sum) (g) (output),label=above:Neuron, label=below:{Warstwa 1}, inner sep=0.65cm] {};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Uczenie regresji logistycznej binarnej:\n",
    "* Model: $$h_{\\theta}(x) = \\sigma(\\sum_{i=0}^n \\theta_ix_i) = P(1|x,\\theta)$$\n",
    "* Funkcja kosztu (entropia krzyżowa): $$\\begin{eqnarray} J(\\theta) &=& -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)}\\log P(1|x^{(i)},\\theta) \\\\ && + (1-y^{(i)})\\log(1-P(1|x^{(i)},\\theta))]\\end{eqnarray}$$\n",
    "\n",
    "* Po obliczeniu $\\nabla J(\\theta)$, zwykły SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Perceptron a \n",
    "## wieloklasowa regresja logistyczna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "input_collapsed": true
   },
   "outputs": [],
   "source": [
    "%%tikz -l arrows,automata,positioning,shapes,shapes.geometric,fit -f png -s 1000,600\n",
    "\\tikzstyle{every node}=[font=\\large]\n",
    "\\tikzstyle{every path}=[line width=1pt]\n",
    "\n",
    "\\node[state] (x0) {$1$};\n",
    "\\node[state] (x1) [below=0.5cm of x0] {$x_1$};\n",
    "\\node[state] (x2) [below=0.5cm of x1] {$x_2$};\n",
    "\\node[state, draw=none,fill=none] (dots) [below=0.5cm of x2] {$\\cdots$};\n",
    "\\node[state] (xn) [below=0.5cm of dots] {$x_n$};\n",
    "\n",
    "\\node[state,circle] (sum1) [right=4cm of x1] {$g(\\sum)$};\n",
    "\\node[state,circle] (sum2) [right=4cm of x2] {$g(\\sum)$};\n",
    "\\node[state,circle] (sum3) [right=4cm of dots] {$g(\\sum)$};\n",
    "\n",
    "\\node[state, draw=none,fill=none] (p1) [right=0.5cm of sum1] {$P(c=0)$};\n",
    "\\node[state, draw=none,fill=none] (p2) [right=0.5cm of sum2] {$P(c=1)$};\n",
    "\\node[state, draw=none,fill=none] (p3) [right=0.5cm of sum3] {$P(c=2)$};\n",
    "\n",
    "\\path[->] \n",
    "(x0) edge node [above, pos=0.5] {$\\theta^{(0)}_{0}$} (sum1)\n",
    "(x1) edge node [above, pos=0.5] {$\\theta^{(0)}_{1}$} (sum1)\n",
    "(x2) edge node [above, pos=0.5] {$\\theta^{(0)}_{2}$} (sum1)\n",
    "(xn) edge node [above, pos=0.5] {$\\theta^{(0)}_{n}$} (sum1);\n",
    "                                     \n",
    "\\path[-, thin, dotted] \n",
    "(x0) edge node {} (sum2)\n",
    "(x1) edge node {} (sum2)\n",
    "(x2) edge node {} (sum2)\n",
    "(xn) edge node {} (sum2)\n",
    "\n",
    "(x0) edge node {} (sum3)\n",
    "(x1) edge node {} (sum3)\n",
    "(x2) edge node {} (sum3)\n",
    "(xn) edge node [below, pos=0.5] {$\\theta^{(2)}_{n}$} (sum3);\n",
    " \n",
    "\\node [draw, dashed, fit= (x0) (x1) (x2) (dots) (xn),label=above:Cechy, label=below:{Warstwa 0}] (w0) {};\n",
    "\\node [draw, dashed, fit= (sum1) (sum2) (sum3), label=below:{Warstwa 1}, label=above:{$g(\\cdot) = \\mathrm{softmax}(\\cdot)$}] (w1) {};\n",
    "\n",
    "\\node[draw=none,fill=none] (theta) [below=1cm of w1] \n",
    "{$\\Theta = \\left[% \n",
    "        \\begin{array}{ccc} %\n",
    "        \\theta_0^{(0)} & \\theta_0^{(1)}  & \\theta_0^{(2)} \\\\%\n",
    "        \\theta_1^{(0)} & \\theta_1^{(1)}  & \\theta_1^{(2)} \\\\%\n",
    "        \\vdots & \\vdots & \\vdots \\\\%\n",
    "        \\theta_n^{(0)} & \\theta_n^{(1)}  & \\theta_n^{(2)} \\\\%\n",
    "        \\end{array} \\right]$\n",
    "};\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "input_collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Wieloklasowa regresji logistyczna\n",
    "* Model (dla $c$ klasyfikatorów binarnych): \n",
    "$$\\begin{eqnarray}\n",
    "h_{(\\theta^{(1)},\\dots,\\theta^{(c)})}(x) &=& \\mathrm{softmax}(\\sum_{i=0}^n \\theta_{i}^{(1)}x_i, \\ldots, \\sum_{i=0}^n \\theta_i^{(c)}x_i) \\\\ \n",
    "&=& \\left[ P(k|x,\\theta^{(1)},\\dots,\\theta^{(c)}) \\right]_{k=1,\\dots,c} \n",
    "\\end{eqnarray}$$\n",
    "* Funkcja kosztu (**przymując model regresji binarnej**): $$\\begin{eqnarray} J(\\theta^{(k)}) &=& -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)}\\log P(k|x^{(i)},\\theta^{(k)}) \\\\ && + (1-y^{(i)})\\log P(\\neg k|x^{(i)},\\theta^{(k)})]\\end{eqnarray}$$\n",
    "\n",
    "* Po obliczeniu $\\nabla J(\\theta)$, **c-krotne** uruchomienie SGD, zastosowanie $\\mathrm{softmax}(X)$ do niezależnie uzyskanych klasyfikatorów binarnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Przyjmijmy: \n",
    "$$ \\Theta = (\\theta^{(1)},\\dots,\\theta^{(c)}) $$\n",
    "\n",
    "$$h_{\\Theta}(x) = \\left[ P(k|x,\\Theta) \\right]_{k=1,\\dots,c}$$\n",
    "\n",
    "$$\\delta(x,y) = \\left\\{\\begin{array}{cl} 1 & \\textrm{gdy } x=y \\\\ 0 & \\textrm{wpp.}\\end{array}\\right.$$\n",
    "\n",
    "* Wieloklasowa funkcja kosztu $J(\\Theta)$ (kategorialna entropia krzyżowa):\n",
    "$$ J(\\Theta) = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{c} \\delta({y^{(i)},k}) \\log P(k|x^{(i)},\\Theta) $$\n",
    "\n",
    "* Gradient $\\nabla J(\\Theta)$:\n",
    "$$ \\dfrac{\\partial J(\\Theta)}{\\partial \\Theta_{j,k}} = -\\frac{1}{m}\\sum_{i = 1}^{m} (\\delta({y^{(i)},k}) - P(k|x^{(i)}, \\Theta)) x^{(i)}_j \n",
    "$$\n",
    "\n",
    "* Liczymy wszystkie wagi jednym uruchomieniem SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Podsumowanie sieci jednowarstwowych\n",
    "\n",
    "* W przypadku jednowarstowej sieci neurnowej wystarczy znać gradient funkcji kosztu.\n",
    "* Wtedy liczymy tak samo jak w przypadku regresji liniowej, logisticznej, wieloklasowej logistycznej itp.\n",
    "* Wymienione modele to szczególne przypadki jednowarstwowych sieci neuronowych.\n",
    "* Regresja liniowa i binarna logistyczna to jeden neuron.\n",
    "* Wieloklasowa regresja logistyczna to tyle neuronów ile klas.\n",
    "* Dobieramy funkcję aktywacji i funkcję kosztu do problemu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "input_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Sieci wielowarstwowe - Przypomnienie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "input_collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cell magic `%%tikz` not found.\n"
     ]
    }
   ],
   "source": [
    "%%tikz -l arrows,automata,positioning,shapes,shapes.geometric,fit -f png -s 1000,600\n",
    "\n",
    "\\node[state] (x1) {$x_1$};\n",
    "\\node[state] (x2) [below=0.5cm of x1] {$x_2$};\n",
    "\\node[state, draw=none,fill=none] (dots) [below=0.5cm of x2] {$\\cdots$};\n",
    "\\node[state] (xn) [below=0.5cm of dots] {$x_n$};\n",
    "\n",
    "\\node[state,circle] (a1) [below right=-0.33cm and 3cm of x1] {$a^{(1)}_1$};\n",
    "\\node[state,circle] (a2) [below=0.5cm of a1] {$a^{(1)}_2$};\n",
    "\\node[state,circle] (a3) [below=0.5cm of a2] {$a^{(1)}_3$};\n",
    "\\node[state] (b1) [above left=1cm and 1cm of a1] {$1$};\n",
    "\n",
    "\\node[state,circle] (a21) [right=2cm of a1] {$a^{(2)}_1$};\n",
    "\\node[state,circle] (a22) [below=0.5cm of a21] {$a^{(2)}_2$};\n",
    "\\node[state,circle] (a23) [below=0.5cm of a22] {$a^{(2)}_3$};\n",
    "\\node[state] (b2) [above left=1cm and 1cm of a21] {$1$};\n",
    "\n",
    "\\node[state,circle] (a31) [right=2cm of a22] {$a^{(3)}_1$};\n",
    "\\node[state] (b3) [right=2cm of b2] {$1$};\n",
    "\n",
    "\\path[-] \n",
    "(b1) edge node [above=.2cm, pos=0.5] {$\\beta^{(1)}_{1}$} (a1)\n",
    "(x1) edge node [above, pos=0.5] {$\\Theta^{(1)}_{1,1}$} (a1)\n",
    "(x2) edge node [above, pos=0.5] {$\\Theta^{(1)}_{2,1}$} (a1)\n",
    "(xn) edge node [above, pos=0.5] {$\\Theta^{(1)}_{n,1}$} (a1);\n",
    "                                     \n",
    "\\path[-, thin, dotted] \n",
    "(b1) edge node {} (a2)\n",
    "(x1) edge node {} (a2)\n",
    "(x2) edge node {} (a2)\n",
    "(xn) edge node {} (a2)\n",
    "\n",
    "(b1) edge node {} (a3)\n",
    "(x1) edge node {} (a3)\n",
    "(x2) edge node {} (a3)\n",
    "(xn) edge node [below, pos=0.5] {$\\Theta^{(1)}_{n,3}$} (a3);\n",
    "\n",
    "\\path[-] \n",
    "(b2) edge node [above=.2cm, pos=0.5] {$\\beta^{(2)}_{1}$} (a21)\n",
    "(a1) edge node [above, pos=0.5] {$\\Theta^{(2)}_{1,1}$} (a21)\n",
    "(a2) edge node [above, pos=0.5] {$\\Theta^{(2)}_{2,1}$} (a21)\n",
    "(a3) edge node [above, pos=0.5] {$\\Theta^{(2)}_{3,1}$} (a21);\n",
    "\n",
    "\\path[-, thin, dotted] \n",
    "(b2) edge node {} (a22)\n",
    "(a1) edge node {} (a22)\n",
    "(a2) edge node {} (a22)\n",
    "(a3) edge node {} (a22)\n",
    "\n",
    "(b2) edge node {} (a23)\n",
    "(a1) edge node {} (a23)\n",
    "(a2) edge node {} (a23)\n",
    "(a3) edge node [below, pos=0.5] {$\\Theta^{(2)}_{3,3}$} (a23);\n",
    "\n",
    "\\path[-] \n",
    "(b3) edge node [above=.5cm, pos=0.5] {$\\beta^{(3)}_{1}$} (a31)\n",
    "(a21) edge node [above, pos=0.5] {$\\Theta^{(3)}_{1,1}$} (a31)\n",
    "(a22) edge node [above, pos=0.5] {$\\Theta^{(3)}_{2,1}$} (a31)\n",
    "(a23) edge node [above, pos=0.5] {$\\Theta^{(3)}_{3,1}$} (a31);\n",
    "\n",
    "\\node [draw, dashed, fit=  (x1) (x2) (dots) (xn)] (w0) {};\n",
    "\\node [draw, dashed, fit= (a1) (a2) (a3)] (w1) {};\n",
    "\\node [draw, dashed, fit= (a21) (a22) (a23)] (w2) {};\n",
    "\\node [draw, dashed, fit= (a31)] (w3) {};\n",
    "\n",
    "\\node [draw, draw=none, fill=none, below=0.5cm of w0] (mw0) {\\small$a^{(0)}=x$};\n",
    "\\node [draw, draw=none, fill=none, right=1.1cm of mw0] (mw1) \n",
    "{\\small$\\begin{array}{l}z^{(1)} = a^{(0)} \\Theta^{(1)} + \\beta^{(1)}\\\\g^{(1)}(x)=\\tanh(x)\\\\a^{(1)}=g^{(1)}(z^{(1)})\\end{array}$};\n",
    "\\node [draw, draw=none, fill=none, right=-.3cm of mw1] (mw2) \n",
    "{\\small$\\begin{array}{l}z^{(2)} = a^{(1)} \\Theta^{(2)} + \\beta^{(2)}\\\\g^{(2)}(x)=\\tanh(x)\\\\a^{(2)}=g^{(2)}(z^{(2)})\\end{array}$};\n",
    "\\node [draw, draw=none, fill=none, right=-.3cm of mw2] (mw3) \n",
    "{\\small$\\begin{array}{l}z^{(3)} = a^{(2)} \\Theta^{(3)} + \\beta^{(3)}\\\\g^{(3)}(x)=\\tanh(x)\\\\a^{(3)}=g^{(3)}(z^{(3)})\\end{array}$};\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "input_collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feedforward 1\n",
    "\n",
    "* Mając daną $n$-warstwową sieć neuronową oraz jej parametry $\\Theta^{(1)}, \\ldots, \\Theta^{(L)} $ oraz $\\beta^{(1)}, \\ldots, \\beta^{(L)} $ liczymy: \n",
    "$$a^{(l)} = g^{(l)}\\left( a^{(l-1)} \\Theta^{(l)} + \\beta^{(l)} \\right). $$ \n",
    "* Parametry $\\Theta$ to wagi na połączeniach miedzy neuronami dwóch warstw. Rozmiar macierzy $\\Theta^{(l)}$, czyli macierzy wag na połączeniach warstw $a^{(l-1)}$ i $a^{(l)}$, to $\\dim(a^{(l-1)}) \\times \\dim(a^{(l)})$. \n",
    "* Parametry $\\beta$ zastępują tutaj dodawanie kolumny z jedynkami do naszej macierzy cech. Macierz $\\beta^{(l)}$ ma rozmiar równy liczbie neuronów w odpowiedniej warstwie, czyli $1 \\times \\dim(a^{(l)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "input_collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Feedforward 2\n",
    "\n",
    "* Funkcje $g^{(l)}$ to tzw. **funkcje aktywacji**.\n",
    "* Dla $i = 0$ przyjmujemy $a^{(0)} = \\mathrm{x}$ (wektor wierszowy cech) oraz $g^{(0)}(x) = x$ (identyczność).\n",
    "* W przypadku klasyfikacji, często dla ostatniej warstwy $L$ (o rozmiarze równym liczbie klas) przyjmuje się $g^{(L)}(x) = \\mathop{\\mathrm{softmax}}(x)$.\n",
    "* Pozostałe funkcje aktywacji najcześciej mają postać sigmoidy (np. funkcja logistyczna lub tangens hiperboliczny, $\\tanh$).\n",
    "* W przypadku regresji często mamy pojedynczy neuron wyjściowy jak na obrazku. Funkcją aktywacji może wtedy być np. funkcja identycznościowa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Uczenie wielowarstwowych sieci \n",
    "Mając algorytm SGD oraz gradienty wszystkich wag, moglibyśmy trenować każdą sieć. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Niech:\n",
    "$$\\Theta = (\\Theta^{(1)},\\Theta^{(2)},\\Theta^{(3)},\\beta^{(1)},\\beta^{(2)},\\beta^{(3)})$$\n",
    "\n",
    "* Funkcja sieci neuronowej z grafiki:\n",
    "\n",
    "$$\\small h_\\Theta(x) = \\tanh(\\tanh(\\tanh(x\\Theta^{(1)}+\\beta^{(1)})\\Theta^{(2)} + \\beta^{(2)})\\Theta^{(3)} + \\beta^{(3)})$$\n",
    "* Funkcja kosztu dla regresji:\n",
    "$$J(\\Theta) = \\dfrac{1}{2m} \\sum_{i=1}^{m} (h_\\Theta(x^{(i)})- y^{(i)})^2 $$\n",
    "* Jak wyglądają gradienty?\n",
    "$$\\nabla_{\\Theta^{(1)}} J(\\Theta) = ? \\quad \\nabla_{\\beta^{(3)}} J(\\Theta) = ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Propagacja wsteczna (Backpropagation)\n",
    "Dla jednego przykładu (x,y):\n",
    "\n",
    "1. **Wejście**: Ustaw aktywacje w warstwie cech $a^{(0)}=x$ \n",
    "2. **Feedforward:** dla $l=1,\\dots,L$ oblicz \n",
    "$$z^{(l)} = a^{(l-1)} \\Theta^{(l)} + \\beta^{(l)} \\textrm{ oraz } a^{(l)}=g^{(l)}(z^{(l)})$$\n",
    "3. **Błąd wyjścia $\\delta^{(L)}$:** oblicz wektor $$\\delta^{(L)}= \\nabla_{a^{(L)}}J(\\Theta) \\odot {g^{\\prime}}^{(L)}(z^{(L)})$$\n",
    "4. **Propagacja wsteczna błędu:** dla $l = L-1,L-2,\\dots,1$ oblicz $$\\delta^{(l)} = \\delta^{(l+1)}(\\Theta^{(l+1)})^T \\odot {g^{\\prime}}^{(l)}(z^{(l)})$$\n",
    "5. **Gradienty:** \n",
    "    * $\\dfrac{\\partial}{\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = a_i^{(l-1)}\\delta_j^{(l)} \\textrm{ oraz } \\dfrac{\\partial}{\\partial \\beta_{j}^{(l)}} J(\\Theta) = \\delta_j^{(l)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\small J(\\Theta) = \\frac{1}{2}(a^{(L)} - y)^2 $$\n",
    "$$\\small  \\dfrac{\\partial}{\\partial a^{(L)}} J(\\Theta) = a^{(L)} - y$$\n",
    "\n",
    "$$\\small \\tanh^{\\prime}(x) = 1 - \\tanh^2(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "input_collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Cell magic `%%tikz` not found.\n"
     ]
    }
   ],
   "source": [
    "%%tikz -l arrows,automata,positioning,shapes,shapes.geometric,fit -f png -s 1000,600\n",
    "\n",
    "\\node[state] (x1) {$x_1$};\n",
    "\\node[state] (x2) [below=0.5cm of x1] {$x_2$};\n",
    "\\node[state, draw=none,fill=none] (dots) [below=0.5cm of x2] {$\\cdots$};\n",
    "\\node[state] (xn) [below=0.5cm of dots] {$x_n$};\n",
    "\n",
    "\\node[state,circle] (a1) [below right=-0.33cm and 3cm of x1] {$a^{(1)}_1$};\n",
    "\\node[state,circle] (a2) [below=0.5cm of a1] {$a^{(1)}_2$};\n",
    "\\node[state,circle] (a3) [below=0.5cm of a2] {$a^{(1)}_3$};\n",
    "\\node[state] (b1) [above left=1cm and 1cm of a1] {$1$};\n",
    "\n",
    "\\node[state,circle] (a21) [right=2cm of a1] {$a^{(2)}_1$};\n",
    "\\node[state,circle] (a22) [below=0.5cm of a21] {$a^{(2)}_2$};\n",
    "\\node[state,circle] (a23) [below=0.5cm of a22] {$a^{(2)}_3$};\n",
    "\\node[state] (b2) [above left=1cm and 1cm of a21] {$1$};\n",
    "\n",
    "\\node[state,circle] (a31) [right=2cm of a22] {$a^{(3)}_1$};\n",
    "\\node[state] (b3) [right=2cm of b2] {$1$};\n",
    "\n",
    "\\node[draw=none, fill=none] (delta3) [below right=0.5cm and -1cm of a31] \n",
    "{$\\delta^{(3)}=(a^{(3)}-y) \\odot (1-\\tanh^2(z^{(3)}))$};\n",
    "\n",
    "\\node[draw=none, fill=none] (delta2) [below right=0.5cm and -1cm of a23] \n",
    "{$\\delta^{(2)}= \\delta^{(3)}(\\Theta^{(3)})^T \\odot (1-\\tanh^2(z^{(2)}))$};\n",
    "\n",
    "\\node[draw=none, fill=none] (delta1) [below right=1.5cm and -4cm of a3] \n",
    "{$\\delta^{(1)}= \\delta^{(2)}(\\Theta^{(2)})^T \\odot (1-\\tanh^2(z^{(1)}))$};\n",
    "\n",
    "\n",
    "\\path[-] \n",
    "(b1) edge node [above=.2cm, pos=0.5] {$\\beta^{(1)}_{1}$} (a1)\n",
    "(x1) edge node [above, pos=0.5] {$\\Theta^{(1)}_{1,1}$} (a1)\n",
    "(x2) edge node [above, pos=0.5] {$\\Theta^{(1)}_{2,1}$} (a1)\n",
    "(xn) edge node [above, pos=0.5] {$\\Theta^{(1)}_{n,1}$} (a1);\n",
    "                                     \n",
    "\\path[-, thin, dotted] \n",
    "(b1) edge node {} (a2)\n",
    "(x1) edge node {} (a2)\n",
    "(x2) edge node {} (a2)\n",
    "(xn) edge node {} (a2)\n",
    "\n",
    "(b1) edge node {} (a3)\n",
    "(x1) edge node {} (a3)\n",
    "(x2) edge node {} (a3)\n",
    "(xn) edge node [below, pos=0.5] {$\\Theta^{(1)}_{n,3}$} (a3);\n",
    "\n",
    "\\path[-] \n",
    "(b2) edge node [above=.2cm, pos=0.5] {$\\beta^{(2)}_{1}$} (a21)\n",
    "(a1) edge node [above, pos=0.5] {$\\Theta^{(2)}_{1,1}$} (a21)\n",
    "(a2) edge node [above, pos=0.5] {$\\Theta^{(2)}_{2,1}$} (a21)\n",
    "(a3) edge node [above, pos=0.5] {$\\Theta^{(2)}_{3,1}$} (a21);\n",
    "\n",
    "\\path[-, thin, dotted] \n",
    "(b2) edge node {} (a22)\n",
    "(a1) edge node {} (a22)\n",
    "(a2) edge node {} (a22)\n",
    "(a3) edge node {} (a22)\n",
    "\n",
    "(b2) edge node {} (a23)\n",
    "(a1) edge node {} (a23)\n",
    "(a2) edge node {} (a23)\n",
    "(a3) edge node [below, pos=0.5] {$\\Theta^{(2)}_{3,3}$} (a23);\n",
    "\n",
    "\\path[-] \n",
    "(b3) edge node [above=.5cm, pos=0.5] {$\\beta^{(3)}_{1}$} (a31)\n",
    "(a21) edge node [above, pos=0.5] {$\\Theta^{(3)}_{1,1}$} (a31)\n",
    "(a22) edge node [above, pos=0.5] {$\\Theta^{(3)}_{2,1}$} (a31)\n",
    "(a23) edge node [above, pos=0.5] {$\\Theta^{(3)}_{3,1}$} (a31);\n",
    "\n",
    "\\node [draw, dashed, fit=  (x1) (x2) (dots) (xn)] (w0) {};\n",
    "\\node [draw, dashed, fit= (a1) (a2) (a3)] (w1) {};\n",
    "\\node [draw, dashed, fit= (a21) (a22) (a23)] (w2) {};\n",
    "\\node [draw, dashed, fit= (a31)] (w3) {};\n",
    "\n",
    "\\node [draw, draw=none, fill=none, below=2cm of w0] (mw0) {\\small$a^{(0)}=x$};\n",
    "\\node [draw, draw=none, fill=none, right=.5cm of mw0] (mw1) \n",
    "{\\small$\\begin{array}{l}z^{(1)} = a^{(0)} \\Theta^{(1)} + \\beta^{(1)}\\\\g^{(1)}(x)=\\tanh(x)\\\\a^{(1)}=g^{(1)}(z^{(1)})\\end{array}$};\n",
    "\\node [draw, draw=none, fill=none, right=.5cm of mw1] (mw2) \n",
    "{\\small$\\begin{array}{l}z^{(2)} = a^{(1)} \\Theta^{(2)} + \\beta^{(2)}\\\\g^{(2)}(x)=\\tanh(x)\\\\a^{(2)}=g^{(2)}(z^{(2)})\\end{array}$};\n",
    "\\node [draw, draw=none, fill=none, right=.5cm of mw2] (mw3) \n",
    "{\\small$\\begin{array}{l}z^{(3)} = a^{(2)} \\Theta^{(3)} + \\beta^{(3)}\\\\g^{(3)}(x)=\\tanh(x)\\\\a^{(3)}=g^{(3)}(z^{(3)})\\end{array}$};\n",
    "\n",
    "\\path[->] \n",
    "(mw0) edge node {} (mw1)\n",
    "(mw1) edge node {} (mw2)\n",
    "(mw2) edge node {} (mw3)\n",
    "(mw3.east) edge[in=320,out=0] node {} (delta3.south)\n",
    ";\n",
    "\n",
    "\\path[->] \n",
    "(delta3) edge node {} (delta2)\n",
    "(delta2) edge node {} (delta1)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SGD z propagacją wsteczną\n",
    "\n",
    "Jedna iteracja:\n",
    "* Dla parametrów $\\Theta = (\\Theta^{(1)},\\ldots,\\Theta^{(L)})$ utwórz pomocnicze macierze zerowe $\\Delta = (\\Delta^{(1)},\\ldots,\\Delta^{(L)})$ o takich samych wymiarach (dla uproszczenia opuszczono wagi $\\beta$).\n",
    "* Dla $m$ przykładów w wsadzie (batch), $i = 1,\\ldots,m$:\n",
    "    * Wykonaj algortym propagacji wstecz dla przykładu $(x^{(i)}, y^{(i)})$ i przechowaj gradienty $\\nabla_{\\Theta}J^{(i)}(\\Theta)$ dla tego przykładu;\n",
    "    * $\\Delta := \\Delta + \\dfrac{1}{m}\\nabla_{\\Theta}J^{(i)}(\\Theta)$\n",
    "* Wykonaj aktualizację wag: $\\Theta := \\Theta - \\alpha \\Delta$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
