{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train multiclass set shape =  (1000, 10)\n",
      "test multiclass set shape =  (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Funkcje narzedziowe i wczytywanie danych - treść zadań jest poniżej\n",
    "\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def read(dataset = \"training\", path = \"../../..\"):\n",
    "    \"\"\"\n",
    "    Python function for importing the MNIST data set.  It returns an iterator\n",
    "    of 2-tuples with the first element being the label and the second element\n",
    "    being a numpy.uint8 2D array of pixel data for the given image.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
    "    else:\n",
    "        raise ValueError, \"dataset must be 'testing' or 'training'\"\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols)\n",
    "\n",
    "    get_img = lambda idx: (lbl[idx], img[idx])\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in xrange(len(lbl)):\n",
    "        yield get_img(i)\n",
    "\n",
    "def show(image):\n",
    "    \"\"\"\n",
    "    Render a given numpy.uint8 2D array of pixel data.\n",
    "    \"\"\"\n",
    "    from matplotlib import pyplot\n",
    "    import matplotlib as mpl\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image, cmap=mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    pyplot.show()\n",
    "    \n",
    "training = list(read(dataset='training'))\n",
    "testing = list(read(dataset='testing'))\n",
    "\n",
    "def toMatrix(data, maxItems=1000):\n",
    "    datalist = [t for t in data]\n",
    "    m = maxItems if maxItems != -1 else len(datalist)\n",
    "    n = 28 * 28 + 1\n",
    "    X = np.matrix(np.zeros(m * n)).reshape(m, n)\n",
    "    Y = np.matrix(np.zeros(m)).reshape(m, 1)\n",
    "    for i, (label, image) in enumerate(datalist[:maxItems]):\n",
    "        X[i, 0] = 255 # bias term\n",
    "        X[i, 1:] = image.reshape(28*28,)\n",
    "        Y[i] = label\n",
    "    return X, Y\n",
    "\n",
    "def digitIndicator(value, digit):\n",
    "    return 1 if value[0] == digit else 0;\n",
    "\n",
    "trainImgMx, trainLblMx = toMatrix(training)\n",
    "testImgMx, testLblMx = toMatrix(testing)\n",
    "\n",
    "zeroCollumnTrain = np.apply_along_axis(digitIndicator, 1, trainLblMx, 0).reshape(trainLblMx.shape)\n",
    "oneCollumnTrain = np.apply_along_axis(digitIndicator, 1, trainLblMx, 1).reshape(trainLblMx.shape)\n",
    "twoCollumnTrain = np.apply_along_axis(digitIndicator, 1, trainLblMx, 2).reshape(trainLblMx.shape)\n",
    "threeCollumnTrain = np.apply_along_axis(digitIndicator, 1, trainLblMx, 3).reshape(trainLblMx.shape)\n",
    "fourCollumnTrain = np.apply_along_axis(digitIndicator, 1, trainLblMx, 4).reshape(trainLblMx.shape)\n",
    "fiveCollumnTrain = np.apply_along_axis(digitIndicator, 1, trainLblMx, 5).reshape(trainLblMx.shape)\n",
    "sixCollumnTrain = np.apply_along_axis(digitIndicator, 1, trainLblMx, 6).reshape(trainLblMx.shape)\n",
    "sevenCollumnTrain = np.apply_along_axis(digitIndicator, 1, trainLblMx, 7).reshape(trainLblMx.shape)\n",
    "eightCollumnTrain = np.apply_along_axis(digitIndicator, 1, trainLblMx, 8).reshape(trainLblMx.shape)\n",
    "nineCollumnTrain = np.apply_along_axis(digitIndicator, 1, trainLblMx, 9).reshape(trainLblMx.shape)\n",
    "\n",
    "trainMultiClassLbl = np.hstack((zeroCollumnTrain, oneCollumnTrain, twoCollumnTrain, threeCollumnTrain,\n",
    "                               fourCollumnTrain, fiveCollumnTrain, sixCollumnTrain, sevenCollumnTrain,\n",
    "                               eightCollumnTrain, nineCollumnTrain))\n",
    "print \"train multiclass set shape = \", trainMultiClassLbl.shape\n",
    "\n",
    "zeroCollumnTest = np.apply_along_axis(digitIndicator, 1, testLblMx, 0).reshape(testLblMx.shape)\n",
    "oneCollumnTest = np.apply_along_axis(digitIndicator, 1, testLblMx, 1).reshape(testLblMx.shape)\n",
    "twoCollumnTest = np.apply_along_axis(digitIndicator, 1, testLblMx, 2).reshape(testLblMx.shape)\n",
    "threeCollumnTest = np.apply_along_axis(digitIndicator, 1, testLblMx, 3).reshape(testLblMx.shape)\n",
    "fourCollumnTest = np.apply_along_axis(digitIndicator, 1, testLblMx, 4).reshape(testLblMx.shape)\n",
    "fiveCollumnTest = np.apply_along_axis(digitIndicator, 1, testLblMx, 5).reshape(testLblMx.shape)\n",
    "sixCollumnTest = np.apply_along_axis(digitIndicator, 1, testLblMx, 6).reshape(testLblMx.shape)\n",
    "sevenCollumnTest = np.apply_along_axis(digitIndicator, 1, testLblMx, 7).reshape(testLblMx.shape)\n",
    "eightCollumnTest = np.apply_along_axis(digitIndicator, 1, testLblMx, 8).reshape(testLblMx.shape)\n",
    "nineCollumnTest = np.apply_along_axis(digitIndicator, 1, testLblMx, 9).reshape(testLblMx.shape)\n",
    "\n",
    "testMultiClassLbl = np.hstack((zeroCollumnTest, oneCollumnTest, twoCollumnTest, threeCollumnTest,\n",
    "                               fourCollumnTest, fiveCollumnTest, sixCollumnTest, sevenCollumnTest,\n",
    "                               eightCollumnTest, nineCollumnTest))\n",
    "print \"test multiclass set shape = \", testMultiClassLbl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania\n",
    "\n",
    "**AdaGrad**\n",
    "1. Rozszerz podany poniżej algorytm SGD o funkcjonalność algorytmu AdaGrad (włączana parametrem `adaGrad=True`). Zasady działania AdaGrad są podane w materiałach do wykładu.<br/> **Uwaga**: podczas dzielenia przez pierwastek sumy kwadratów historycznych gradientów warto dodać małą wartość $\\epsilon=10^{-7}$ do mianownika (żeby nie dzielić przez 0).\n",
    "1. Wykorzystując dane z ostatnich ćwiczeń oraz algorytm AdaGrad, stwórz model wieloklasowej regresji logistycznej (dla wszystkich 10) z parametrami:\n",
    "    * Rozmiar wsadu: 50\n",
    "    * Liczba epok: 2\n",
    "    * Rozmiar kroku $\\alpha$: 1.0 (gdy używamy AdaGrad $\\alpha$ może być niezmienne)\n",
    "* Oblicz jego jakość na zbiorze testowym. Następnie sprobuj uzyskać wynik podobny lub lepszy samym algorytmem SGD bez opcji AdaGrad, modyfikująć paramtr $\\alpha$. Da się?\n",
    "\n",
    "**Walidacja Krzyżowa**\n",
    "1. Na podstawie pseudo-kodu z wykładu zaimplementuj 5-krotną walidację krzyżową na danych uczących. Wykorzystaj średnią poprawność klasyfikacyjną z 5-ciu zbiorów walidacyjnych jako ocenę jakości na całym zbiorze uczącym.\n",
    "2. Dobierz na podstawie walidacji krzyżowej najlepsze parametry modelu (rozmiar wsadu, liczba epok, użycie SGD z/bez AdaGrad, paramtr $\\alpha$), **nie (!)** patrząć na zbiór testowy.\n",
    "3. Sprawdź, czy tak dobrany model, po przetrenowaniu na całych danych jest również najlepszy na zbiorze testowym. (Nie ma niestety takiej gwarancji.)\n",
    "\n",
    "**Ensemble** (za dodatkowe 20 punktów)\n",
    "1. Wytrenuj 10 klasyfikatorów z powyżej dobranymi parametrami tasujać dane trenujące przed każdym trenowaniem (pamiętaj by tasować odpowiedzi w tej samej kolejności!). \n",
    "1. Oblicz predykcje (klasy) dla każdego z nich na zbiorze testowym oraz jakość każdego klasyfikatora.\n",
    "1. Dla każdego przykłady wybierz klasę, która występowała najczęśćiej w odpowiedziach 10 klasyfikatorów jako nową klasę. \n",
    "1. Oblicz jakość tak uzyskanych predykcji i porównaj z jakością pojedynczych klasyfikatorów. Pomogło?\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def runningMeanFast(x, N):\n",
    "    return np.convolve(x, np.ones((N,))/N, mode='valid')\n",
    "\n",
    "def safeSigmoid(x, eps=0):\n",
    "    y = 1.0/(1.0 + np.exp(-x))\n",
    "    # przytnij od dolu i gory\n",
    "    if eps > 0:\n",
    "        y[y < eps] = eps\n",
    "        y[y > 1 - eps] = 1 - eps\n",
    "    return y\n",
    "\n",
    "def h(theta, X, eps=0.0):\n",
    "    return safeSigmoid(X*theta, eps)\n",
    "\n",
    "def J(h,theta,X,y):\n",
    "    m = len(y)\n",
    "    f = h(theta, X, eps=10**-7)\n",
    "    return -np.sum(np.multiply(y, np.log(f)) + \n",
    "                   np.multiply(1 - y, np.log(1 - f)), axis=0)/m\n",
    "\n",
    "def dJ(h,theta,X,y):\n",
    "    return 1.0/len(y)*(X.T*(h(theta,X)-y))\n",
    "\n",
    "def softmax(X):\n",
    "    return np.exp(X)/np.sum(np.exp(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrzej/.local/lib/python2.7/site-packages/ipykernel/__main__.py:7: RuntimeWarning: overflow encountered in exp\n",
      "/home/andrzej/.local/lib/python2.7/site-packages/ipykernel/__main__.py:27: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len thetas =  10\n",
      "(1000, 1)\n",
      "Accuracy AdaGrad = 0.694\n",
      "Accuracy No Ada= 0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrzej/.local/lib/python2.7/site-packages/ipykernel/__main__.py:27: RuntimeWarning: invalid value encountered in divide\n"
     ]
    }
   ],
   "source": [
    "def SGD(h, fJ, fdJ, theta, X, y, \n",
    "        alpha=0.001, maxEpochs=1.0, batchSize=100, adaGrad=False):\n",
    "    m, n = X.shape\n",
    "    start, end = 0, batchSize\n",
    "    eps = 10**-7\n",
    "    hgrad = np.matrix(np.zeros(n)).reshape(n, 1)\n",
    "    maxSteps = (m * float(maxEpochs)) / batchSize\n",
    "    for i in range(int(maxSteps)):\n",
    "        XBatch, yBatch =  X[start:end,:], y[start:end,:]\n",
    "        grad = fdJ(h, theta, XBatch, yBatch)\n",
    "        if adaGrad:\n",
    "            hgrad += np.multiply(grad, grad)\n",
    "            adjusted_grad = np.multiply(alpha/(np.sqrt(hgrad)+eps), grad)\n",
    "        else:\n",
    "            adjusted_grad = alpha * grad       \n",
    "        theta = theta - adjusted_grad\n",
    "        if start + batchSize < m:\n",
    "            start += batchSize\n",
    "        else:\n",
    "            start = 0\n",
    "        end = min(start + batchSize, m)\n",
    "    return theta\n",
    "\n",
    "def trainMaxEnt(X, Y, alpha=0.001, maxEpochs=1.0, batchSize=100, adaGrad=False):\n",
    "    n = X.shape[1]\n",
    "    thetas = []\n",
    "    for c in range(Y.shape[1]):\n",
    "        YBi = Y[:,c]\n",
    "        YBi = YBi.reshape(YBi.shape[0], 1)\n",
    "        theta = np.matrix(np.random.random(n)).reshape(n,1)\n",
    "        thetaBest = SGD(h, J, dJ, theta, X, YBi, alpha, maxEpochs, batchSize, adaGrad)\n",
    "        thetas.append(thetaBest)\n",
    "    return thetas\n",
    "\n",
    "def classify(thetas, X):\n",
    "    regs = np.array([(X*theta).item() for theta in thetas])\n",
    "    probs = softmax(regs)\n",
    "    return np.argmax(probs), probs\n",
    "\n",
    "thetas = trainMaxEnt(trainImgMx, trainMultiClassLbl, alpha=1, maxEpochs=2, batchSize=50, adaGrad=True);\n",
    "print \"len thetas = \", len(thetas)\n",
    "testCls = testMultiClassLbl * np.matrix((0, 1, 2, 3, 4, 5, 6, 7, 8, 9)).T\n",
    "\n",
    "print testCls.shape\n",
    "\n",
    "acc = 0.0\n",
    "for i in range(len(testCls)):\n",
    "    cls, probs = classify(thetas, testImgMx[i])\n",
    "    correct = int(testCls[i].item())\n",
    "    acc += correct == cls\n",
    "print \"Accuracy AdaGrad =\", acc/float(len(testImgMx))\n",
    "\n",
    "thetasNoAda = trainMaxEnt(trainImgMx, trainMultiClassLbl, alpha=0.01, maxEpochs=2, batchSize=50, adaGrad=False);\n",
    "\n",
    "accNoAda = 0.0\n",
    "for i in range(len(testCls)):\n",
    "    cls, probs = classify(thetasNoAda, testImgMx[i])\n",
    "    correct = int(testCls[i].item())\n",
    "    accNoAda += correct == cls\n",
    "print \"Accuracy No Ada=\", accNoAda/float(len(testImgMx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dla odpowiednio zmienionego parametru alfa, nawet bez opcji AdaGrad, otrzymaliśmy podobnej jakości wynik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
